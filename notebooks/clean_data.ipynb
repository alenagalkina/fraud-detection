{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98728805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, hour, minute, second\n",
    "from pyspark.sql.types import TimestampType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed76c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "args = \"hdfs dfs -ls | awk '{print $8}'\"\n",
    "proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "s_output, s_err = proc.communicate()\n",
    "all_dart_dirs = s_output.split()\n",
    "all_dart_dirs = all_dart_dirs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4abc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS_hw3\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af0ca6",
   "metadata": {},
   "source": [
    "Типы некорректных данных:\n",
    "1. (обнаружено) сustomer_id=-999999, сustomer_id=999999\n",
    "2. (обнаружено) tx_amount=0\n",
    "3. (обнаружено) transaction_id: дубликаты\n",
    "4. (проверка) tx_datetime: значения, которые выпадают за требуемые ограничения (часы, минуты и т.п.)\n",
    "5. (проверка) tx_fraud: вне [0, 1], tx_fraud_scenario: вне [0, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c9f65",
   "metadata": {},
   "source": [
    "### Создать скрипт, который должен выполнять очистку данных с использованием Apache Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815224cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(path):\n",
    "    data = spark.read.text(path)\n",
    "    header=data.first()[0]\n",
    "    schema=header.split(\" | \")\n",
    "    df_new = data.filter(data[\"value\"] != header).rdd.map(lambda x:x[0].split(\",\")).toDF(schema)\n",
    "    df_new = df_new.withColumn(\"tx_datetime\", df_new.tx_datetime.cast(TimestampType()))\\\n",
    "        .withColumn(\"customer_id\", df_new.customer_id.cast(IntegerType()))\\\n",
    "        .withColumn(\"terminal_id\", df_new.terminal_id.cast(IntegerType()))\\\n",
    "        .withColumn(\"tx_amount\", df_new.tx_amount.cast(DoubleType()))\\\n",
    "        .withColumn(\"tx_time_seconds\", df_new.tx_time_seconds.cast(IntegerType()))\\\n",
    "        .withColumn(\"tx_time_days\", df_new.tx_time_days.cast(IntegerType()))\\\n",
    "        .withColumn(\"tx_fraud\", df_new.tx_fraud.cast(IntegerType()))\\\n",
    "        .withColumn(\"tx_fraud_scenario\", df_new.tx_fraud_scenario.cast(IntegerType()))\n",
    "    \n",
    "    df_new = df_new.withColumn(\"hour\", hour(col(\"tx_datetime\")))\\\n",
    "        .withColumn(\"minute\", minute(col(\"tx_datetime\")))\\\n",
    "        .withColumn(\"second\", second(col(\"tx_datetime\")))\n",
    "\n",
    "    df_new = df_new.dropDuplicates([\"# tranaction_id\"])\n",
    "    df_new = df_new.filter((df_new[\"customer_id\"] != -999999) & \n",
    "                           (df_new[\"customer_id\"] != 999999) &\n",
    "                           (df_new[\"tx_amount\"] != 0)\n",
    "                          )\n",
    "    \n",
    "    data.write.parquet(\"output/\" + path[:-4] + \".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0a591",
   "metadata": {},
   "source": [
    "### Выполнить очистку датасета с использованием созданного скрипта и сохранить его в bucket’е в формате parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9457b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in all_dart_dirs:\n",
    "    clean_data(str(path)[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e200a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a817d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
